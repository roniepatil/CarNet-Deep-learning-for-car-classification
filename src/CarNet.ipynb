{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNee4lmUrEeixKDj6K7QkJ1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"0f1f3348991d4ba58f34ad64506c9b33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11adab5390b441a8811ad5b5833adc37","IPY_MODEL_1768c1958e664ab5b865aa82937a97ac","IPY_MODEL_5c6044c6fd1c448fbf7b90ffa5a60af3"],"layout":"IPY_MODEL_369da08d7e394b688240a7398ea883e0"}},"11adab5390b441a8811ad5b5833adc37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfe35b6ec4cb4e8bba5376f82d6dab3c","placeholder":"​","style":"IPY_MODEL_3ca1ee9b09cd4b48bfd57777910832b3","value":"  0%"}},"1768c1958e664ab5b865aa82937a97ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_093808660f064de5a57b3a5f1a5073e4","max":255,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0702543477c7409ba03e4c8c5175b673","value":0}},"5c6044c6fd1c448fbf7b90ffa5a60af3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1556b95a38f040fc8352ac997c41880c","placeholder":"​","style":"IPY_MODEL_473ded391ce442ea9ed468ab5ecedbaa","value":" 0/255 [00:10&lt;?, ?it/s]"}},"369da08d7e394b688240a7398ea883e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfe35b6ec4cb4e8bba5376f82d6dab3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ca1ee9b09cd4b48bfd57777910832b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"093808660f064de5a57b3a5f1a5073e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0702543477c7409ba03e4c8c5175b673":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1556b95a38f040fc8352ac997c41880c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"473ded391ce442ea9ed468ab5ecedbaa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Mount drive"],"metadata":{"id":"YQe7Q-B_F-Rs"}},{"cell_type":"code","source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n","FOLDERNAME = 'enpm809k/CarNet/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qe8XZXOty5FH","executionInfo":{"status":"ok","timestamp":1670908123046,"user_tz":300,"elapsed":21479,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}},"outputId":"35b4fdc5-0b6b-4930-b1e5-baba98aff951"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/enpm809k/CarNet\n"]}]},{"cell_type":"markdown","source":["Import dependencies"],"metadata":{"id":"SebXuJ_NF4_B"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"umm9wTgiKl5K","executionInfo":{"status":"ok","timestamp":1670908141512,"user_tz":300,"elapsed":16393,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"outputs":[],"source":["import argparse\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","from tqdm.auto import tqdm\n","\n","# from src.model import build_model\n","# from datasets import get_datasets, get_data_loaders\n","# from utils import save_model, save_plots\n","# from loss import LabelSmoothingCrossEntropy\n","\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from torchvision import utils\n","import numpy as np\n","import cv2\n","import PIL\n","from PIL import Image\n","torch.backends.cuda.matmul.allow_tf32 = False\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.allow_tf32 = True\n","data = torch.randn([32, 352, 7, 7], dtype=torch.float, device='cuda', requires_grad=True)\n","net = torch.nn.Conv2d(352, 1408, kernel_size=[1, 1], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)\n","net = net.cuda().float()\n","out = net(data)\n","out.backward(torch.randn_like(out))\n","torch.cuda.synchronize()\n","seed = 42\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True\n"]},{"cell_type":"markdown","source":["Dataset preparation and augmentation"],"metadata":{"id":"yJE_PiJBGaYu"}},{"cell_type":"code","source":["# Required constants.\n","TRAIN_DIR = 'input/car_data/car_data/train/'\n","VALID_DIR = 'input/car_data/car_data/test/'\n","IMAGE_SIZE = 224 # Image size of resize when applying transforms.\n","BATCH_SIZE = 32\n","NUM_WORKERS = 2 # Number of parallel processes for data preparation.\n","\n","# Training transforms\n","def get_train_transform(IMAGE_SIZE):\n","    train_transform = transforms.Compose([\n","        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomRotation(35),\n","        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n","        # transforms.RandomGrayscale(p=0.5),\n","        transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n","        # transforms.RandomPosterize(bits=2, p=0.5),\n","        transforms.ToTensor(),\n","        # transforms.Normalize(\n","        #     mean=[0.485, 0.456, 0.406],\n","        #     std=[0.229, 0.224, 0.225]\n","        #     )\n","    ])\n","    return train_transform\n","\n","# Validation transforms\n","def get_valid_transform(IMAGE_SIZE):\n","    valid_transform = transforms.Compose([\n","        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","        transforms.ToTensor(),\n","        # transforms.Normalize(\n","        #     mean=[0.485, 0.456, 0.406],\n","        #     std=[0.229, 0.224, 0.225]\n","        #     )\n","    ])\n","    return valid_transform\n","\n","def get_datasets():\n","    \"\"\"\n","    Function to prepare the Datasets.\n","    Returns the training and validation datasets along \n","    with the class names.\n","    \"\"\"\n","    dataset_train = datasets.ImageFolder(\n","        TRAIN_DIR, \n","        transform=(get_train_transform(IMAGE_SIZE))\n","    )\n","    dataset_valid = datasets.ImageFolder(\n","        VALID_DIR, \n","        transform=(get_valid_transform(IMAGE_SIZE))\n","    )\n","    return dataset_train, dataset_valid, dataset_train.classes\n","\n","def get_data_loaders(dataset_train, dataset_valid):\n","    \"\"\"\n","    Prepares the training and validation data loaders.\n","    :param dataset_train: The training dataset.\n","    :param dataset_valid: The validation dataset.\n","    Returns the training and validation data loaders.\n","    \"\"\"\n","    train_loader = DataLoader(\n","        dataset_train, batch_size=BATCH_SIZE, \n","        shuffle=True, num_workers=NUM_WORKERS\n","    )\n","    valid_loader = DataLoader(\n","        dataset_valid, batch_size=BATCH_SIZE, \n","        shuffle=False, num_workers=NUM_WORKERS\n","    )\n","    return train_loader, valid_loader \n","\n","dt, dv, classes = get_datasets()\n","tl, vl = get_data_loaders(dt, dv)\n","images, labels = next(iter(tl))\n","# print(images[0].size())\n","a = transforms.ToPILImage()(images[0])"],"metadata":{"id":"p-_htlJpGgoS","executionInfo":{"status":"ok","timestamp":1670908205368,"user_tz":300,"elapsed":63317,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Utility functions"],"metadata":{"id":"rpQOG-A9IGZp"}},{"cell_type":"code","source":["import matplotlib\n","matplotlib.style.use('ggplot')\n","\n","def save_model(epochs, model, optimizer, criterion):\n","    \"\"\"\n","    Function to save the trained model to disk.\n","    \"\"\"\n","    torch.save({\n","                'epoch': epochs,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': criterion,\n","                }, f\"/outputs/model.pth\")\n","\n","def save_plots(train_acc, valid_acc, train_loss, valid_loss, learning_rate):\n","    \"\"\"\n","    Function to save the loss and accuracy plots to disk.\n","    \"\"\"\n","    # Accuracy plots.\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(\n","        train_acc, color='green', linestyle='-', \n","        label='train accuracy'\n","    )\n","    plt.plot(\n","        valid_acc, color='blue', linestyle='-', \n","        label='validataion accuracy'\n","    )\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.savefig(f\"/outputs/accuracy.png\")\n","    \n","    # Loss plots.\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(\n","        train_loss, color='orange', linestyle='-', \n","        label='train loss'\n","    )\n","    plt.plot(\n","        valid_loss, color='red', linestyle='-', \n","        label='validataion loss'\n","    )\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig(f\"/outputs/loss.png\")\n","\n","    # LR plots.\n","    plt.figure(figsize=(10, 7))\n","    \n","    plt.plot(\n","        learning_rate, color='blue', linestyle='-', \n","        label='learning rate'\n","    )\n","    plt.xlabel('Epochs')\n","    plt.ylabel('LR')\n","    plt.legend()\n","    plt.savefig(f\"/outputs/LR.png\")"],"metadata":{"id":"o5gMYIFZIa6B","executionInfo":{"status":"ok","timestamp":1670908205368,"user_tz":300,"elapsed":2,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Decoder Network"],"metadata":{"id":"bfyzz_rUJWWv"}},{"cell_type":"code","source":["from typing import Optional\n","\n","import torch\n","from torch import nn, Tensor\n","from torch.nn.modules.transformer import _get_activation_fn\n","\n","class TransformerDecoderLayerOptimal(nn.Module):\n","    def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n","                 layer_norm_eps=1e-5) -> None:\n","        super(TransformerDecoderLayerOptimal, self).__init__()\n","        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n","        self.dropout = nn.Dropout(dropout)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.dropout3 = nn.Dropout(dropout)\n","\n","        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","\n","        # Implementation of Feedforward model\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","\n","        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n","        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n","\n","        self.activation = _get_activation_fn(activation)\n","\n","    def __setstate__(self, state):\n","        if 'activation' not in state:\n","            state['activation'] = torch.nn.functional.relu\n","        super(TransformerDecoderLayerOptimal, self).__setstate__(state)\n","\n","    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n","                memory_mask: Optional[Tensor] = None,\n","                tgt_key_padding_mask: Optional[Tensor] = None,\n","                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n","        tgt = tgt + self.dropout1(tgt)\n","        tgt = self.norm1(tgt)\n","        tgt2 = self.multihead_attn(tgt, memory, memory)[0]\n","        tgt = tgt + self.dropout2(tgt2)\n","        tgt = self.norm2(tgt)\n","        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n","        tgt = tgt + self.dropout3(tgt2)\n","        tgt = self.norm3(tgt)\n","        return tgt\n","\n","\n","# @torch.jit.script\n","# class ExtrapClasses(object):\n","#     def __init__(self, num_queries: int, group_size: int):\n","#         self.num_queries = num_queries\n","#         self.group_size = group_size\n","#\n","#     def __call__(self, h: torch.Tensor, class_embed_w: torch.Tensor, class_embed_b: torch.Tensor, out_extrap:\n","#     torch.Tensor):\n","#         # h = h.unsqueeze(-1).expand(-1, -1, -1, self.group_size)\n","#         h = h[..., None].repeat(1, 1, 1, self.group_size) # torch.Size([bs, 5, 768, groups])\n","#         w = class_embed_w.view((self.num_queries, h.shape[2], self.group_size))\n","#         out = (h * w).sum(dim=2) + class_embed_b\n","#         out = out.view((h.shape[0], self.group_size * self.num_queries))\n","#         return out\n","\n","# @torch.jit.script\n","class GroupFC(object):\n","    def __init__(self, embed_len_decoder: int):\n","        self.embed_len_decoder = embed_len_decoder\n","\n","    def __call__(self, h: torch.Tensor, duplicate_pooling: torch.Tensor, out_extrap: torch.Tensor):\n","        for i in range(h.shape[1]):\n","            h_i = h[:, i, :]\n","            if len(duplicate_pooling.shape)==3:\n","                w_i = duplicate_pooling[i, :, :]\n","            else:\n","                w_i = duplicate_pooling\n","            out_extrap[:, i, :] = torch.matmul(h_i, w_i)\n","\n","\n","class MLDecoder(nn.Module):\n","    def __init__(self, num_classes, num_of_groups=-1, decoder_embedding=768,\n","                 initial_num_features=1408, zsl=0):\n","        super(MLDecoder, self).__init__()\n","        embed_len_decoder = 100 if num_of_groups < 0 else num_of_groups\n","        if embed_len_decoder > num_classes:\n","            embed_len_decoder = num_classes\n","        # print('embed_len_decoder')\n","        # print(embed_len_decoder)\n","\n","        # switching to 768 initial embeddings\n","        decoder_embedding = 768 if decoder_embedding < 0 else decoder_embedding\n","        embed_standart = nn.Linear(initial_num_features, decoder_embedding)\n","\n","        # non-learnable queries\n","        if not zsl:\n","            query_embed = nn.Embedding(embed_len_decoder, decoder_embedding)\n","            query_embed.requires_grad_(False)\n","            # print('query_embed')\n","            # print(query_embed)\n","        else:\n","            query_embed = None\n","\n","        # decoder\n","        decoder_dropout = 0.1\n","        num_layers_decoder = 1\n","        dim_feedforward = 2048\n","        layer_decode = TransformerDecoderLayerOptimal(d_model=decoder_embedding,\n","                                                      dim_feedforward=dim_feedforward, dropout=decoder_dropout)\n","        self.decoder = nn.TransformerDecoder(layer_decode, num_layers=num_layers_decoder)\n","        self.decoder.embed_standart = embed_standart\n","        self.decoder.query_embed = query_embed\n","        self.zsl = zsl\n","\n","        if self.zsl:\n","            if decoder_embedding != 300:\n","                self.wordvec_proj = nn.Linear(300, decoder_embedding)\n","            else:\n","                self.wordvec_proj = nn.Identity()\n","            self.decoder.duplicate_pooling = torch.nn.Parameter(torch.Tensor(decoder_embedding, 1))\n","            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(1))\n","            self.decoder.duplicate_factor = 1\n","        else:\n","            # group fully-connected\n","            # print('GFC')\n","            self.decoder.num_classes = num_classes\n","            self.decoder.duplicate_factor = int(num_classes / embed_len_decoder + 0.999)\n","            self.decoder.duplicate_pooling = torch.nn.Parameter(\n","                torch.Tensor(embed_len_decoder, decoder_embedding, self.decoder.duplicate_factor))\n","            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(num_classes))\n","        torch.nn.init.xavier_normal_(self.decoder.duplicate_pooling)\n","        torch.nn.init.constant_(self.decoder.duplicate_pooling_bias, 0)\n","        self.decoder.group_fc = GroupFC(embed_len_decoder)\n","        self.train_wordvecs = None\n","        self.test_wordvecs = None\n","\n","    def forward(self, x):\n","        if len(x.shape) == 4:  # [bs,2048, 7,7]\n","            embedding_spatial = x.flatten(2).transpose(1, 2)\n","        else:  # [bs, 197,468]\n","            embedding_spatial = x\n","        embedding_spatial_786 = self.decoder.embed_standart(embedding_spatial)\n","        embedding_spatial_786 = torch.nn.functional.relu(embedding_spatial_786, inplace=True)\n","\n","        bs = embedding_spatial_786.shape[0]\n","        if self.zsl:\n","            query_embed = torch.nn.functional.relu(self.wordvec_proj(self.decoder.query_embed))\n","        else:\n","            query_embed = self.decoder.query_embed.weight\n","        # tgt = query_embed.unsqueeze(1).repeat(1, bs, 1)\n","        tgt = query_embed.unsqueeze(1).expand(-1, bs, -1)  # no allocation of memory with expand\n","        h = self.decoder(tgt, embedding_spatial_786.transpose(0, 1))  # [embed_len_decoder, batch, 768]\n","        h = h.transpose(0, 1)\n","\n","        out_extrap = torch.zeros(h.shape[0], h.shape[1], self.decoder.duplicate_factor, device=h.device, dtype=h.dtype)\n","        self.decoder.group_fc(h, self.decoder.duplicate_pooling, out_extrap)\n","        if not self.zsl:\n","            # print('flatten num classes')\n","            h_out = out_extrap.flatten(1)[:, :self.decoder.num_classes]\n","        else:\n","            h_out = out_extrap.flatten(1)\n","        h_out += self.decoder.duplicate_pooling_bias\n","        # print('h_out')\n","        # print(h_out)\n","        logits = h_out\n","        return logits"],"metadata":{"id":"km0tOLeLJaHk","executionInfo":{"status":"ok","timestamp":1670908205368,"user_tz":300,"elapsed":2,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Backbone network: Efficient Net B2 + MLDecoder classifier"],"metadata":{"id":"r4GmwiMyIzc_"}},{"cell_type":"code","source":["!pip install efficientnet_pytorch\n","\n","from efficientnet_pytorch import EfficientNet\n","\n","class EfficientNet_b5(nn.Module):\n","    def __init__(self):\n","        super(EfficientNet_b5,self).__init__()\n","        self.model = EfficientNet.from_pretrained('efficientnet-b5')\n","        self.classifier_layer = nn.Sequential(\n","            MLDecoder(num_classes=196, initial_num_features=2048)\n","        )\n","    \n","    def forward(self,inputs):\n","        x = self.model.extract_features(inputs)\n","        x = self.classifier_layer(x)\n","        return x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mU4eYuz6I8x7","executionInfo":{"status":"ok","timestamp":1670908526114,"user_tz":300,"elapsed":2982,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}},"outputId":"ce7323c4-e8be-4c75-ecec-37eda9b4baae"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.8/dist-packages (0.7.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from efficientnet_pytorch) (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->efficientnet_pytorch) (4.4.0)\n"]}]},{"cell_type":"markdown","source":["Build model"],"metadata":{"id":"n28ExVcmKdOb"}},{"cell_type":"code","source":["def build_model(pretrained=True, fine_tune=True, num_classes=196):\n","    Num_classes = num_classes\n","    if pretrained:\n","        print('[INFO]: Loading pre-trained weights')\n","    else:\n","        print('[INFO]: Not loading pre-trained weights')\n","    # model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)\n","    # model = models.efficientnet_b7(pretrained=pretrained)\n","    model = EfficientNet_b5()\n","\n","    if fine_tune:\n","        print('[INFO]: Fine-tuning all layers...')\n","        for params in model.parameters():\n","            params.requires_grad = True\n","    elif not fine_tune:\n","        print('[INFO]: Freezing hidden layers...')\n","        for params in model.parameters():\n","            params.requires_grad = False\n","    return model"],"metadata":{"id":"qT3OHI8GKei0","executionInfo":{"status":"ok","timestamp":1670908418403,"user_tz":300,"elapsed":116,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Loss function"],"metadata":{"id":"5PwzFRo8Kpe1"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def reduce_loss(loss, reduction='mean'):\n","    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss\n","\n","\n","def linear_combination(x, y, epsilon):\n","    return epsilon * x + (1 - epsilon) * y\n","\n","\n","class LabelSmoothingCrossEntropy(nn.Module):\n","    def __init__(self, epsilon: float = 0.1, reduction='mean'):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        self.reduction = reduction\n","\n","    def forward(self, preds, target):\n","        n = preds.size()[-1]\n","        log_preds = F.log_softmax(preds, dim=-1)\n","        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n","        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n","        return linear_combination(loss / n, nll, self.epsilon)"],"metadata":{"id":"VUFTAzNGKrY-","executionInfo":{"status":"ok","timestamp":1670908422757,"user_tz":300,"elapsed":1,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Train and validate functions"],"metadata":{"id":"3zSRG9PtK7jY"}},{"cell_type":"code","source":["def train(model, trainloader, optimizer, criterion):\n","    model.train()\n","    print('Training')\n","    train_running_loss = 0.0\n","    train_running_correct = 0\n","    counter = 0\n","    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n","        counter += 1\n","        image, labels = data\n","        image = image.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        # Forward pass.\n","        outputs = model(image)\n","        # Calculate the loss.\n","        loss = criterion(outputs, labels)\n","        train_running_loss += loss.item()\n","        # Calculate the accuracy.\n","        _, preds = torch.max(outputs.data, 1)\n","        train_running_correct += (preds == labels).sum().item()\n","        # Backpropagation.\n","        loss.backward()\n","        # Update the weights.\n","        optimizer.step()\n","    \n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = train_running_loss / counter\n","    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n","    return epoch_loss, epoch_acc\n","\n","# Validation function.\n","def validate(model, testloader, criterion, class_names):\n","    model.eval()\n","    print('Validation')\n","    valid_running_loss = 0.0\n","    valid_running_correct = 0\n","    counter = 0\n","\n","    with torch.no_grad():\n","        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n","            counter += 1\n","            \n","            image, labels = data\n","            image = image.to(device)\n","            labels = labels.to(device)\n","            # Forward pass.\n","            outputs = model(image)\n","            # Calculate the loss.\n","            loss = criterion(outputs, labels)\n","            valid_running_loss += loss.item()\n","            # Calculate the accuracy.\n","            _, preds = torch.max(outputs.data, 1)\n","            valid_running_correct += (preds == labels).sum().item()\n","        \n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = valid_running_loss / counter\n","    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n","    return epoch_loss, epoch_acc"],"metadata":{"id":"p91KGJgLK9aG","executionInfo":{"status":"ok","timestamp":1670908434941,"user_tz":300,"elapsed":169,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Execute training"],"metadata":{"id":"24LRNYGjLOhY"}},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"cGQdFtZA6Pxl","executionInfo":{"status":"ok","timestamp":1670908664276,"user_tz":300,"elapsed":143,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["dataset_train, dataset_valid, dataset_classes = get_datasets()\n","print(f\"[INFO]: Number of training images: {len(dataset_train)}\")\n","print(f\"[INFO]: Number of validation images: {len(dataset_valid)}\")\n","# Load the training and validation data loaders.\n","train_loader, valid_loader = get_data_loaders(dataset_train, dataset_valid)\n","\n","# Learning_parameters. \n","# lr = args['learning_rate']\n","lr = 0.0001\n","# epochs = args['epochs']\n","epochs = 1\n","device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Computation device: {device}\")\n","print(f\"Learning rate: {lr}\")\n","print(f\"Epochs to train for: {epochs}\\n\")\n","\n","# Load the model.\n","model = build_model(\n","    pretrained=True,\n","    fine_tune=True, \n","    num_classes=len(dataset_classes)\n",").to(device)\n","    \n","# Total parameters and trainable parameters.\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"{total_params:,} total parameters.\")\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params:,} training parameters.\")\n","\n","# Optimizer.\n","optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=2e-1)\n","# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3)\n","# Loss function.\n","# criterion = nn.CrossEntropyLoss()\n","criterion = LabelSmoothingCrossEntropy()\n","\n","# Lists to keep track of losses and accuracies.\n","train_loss, valid_loss = [], []\n","train_acc, valid_acc = [], []\n","learning_rate = []\n","# Start the training.\n","for epoch in range(epochs):\n","    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n","    print(optimizer.param_groups[0]['lr'])\n","    learning_rate.append(optimizer.param_groups[0]['lr'])\n","    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n","                                            optimizer, criterion)\n","    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n","                                                    criterion, dataset_classes)\n","    # scheduler.step(valid_epoch_loss)\n","    train_loss.append(train_epoch_loss)\n","    valid_loss.append(valid_epoch_loss)\n","    train_acc.append(train_epoch_acc)\n","    valid_acc.append(valid_epoch_acc)\n","    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n","    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n","    print('-'*50)\n","    time.sleep(2)\n","\n","# Save the trained model weights.\n","# save_model(epochs, model, optimizer, criterion)\n","# Save the loss and accuracy plots.\n","# save_plots(train_acc, valid_acc, train_loss, valid_loss, learning_rate)\n","print('TRAINING COMPLETE')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":672,"referenced_widgets":["0f1f3348991d4ba58f34ad64506c9b33","11adab5390b441a8811ad5b5833adc37","1768c1958e664ab5b865aa82937a97ac","5c6044c6fd1c448fbf7b90ffa5a60af3","369da08d7e394b688240a7398ea883e0","cfe35b6ec4cb4e8bba5376f82d6dab3c","3ca1ee9b09cd4b48bfd57777910832b3","093808660f064de5a57b3a5f1a5073e4","0702543477c7409ba03e4c8c5175b673","1556b95a38f040fc8352ac997c41880c","473ded391ce442ea9ed468ab5ecedbaa"]},"id":"FUlL1jbHLR1X","executionInfo":{"status":"error","timestamp":1670908598733,"user_tz":300,"elapsed":12660,"user":{"displayName":"Rohit Mrutyunjayagouda Patil","userId":"02765913629516684635"}},"outputId":"27b44288-645e-4d40-881c-bd3c1b9d47d8"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]: Number of training images: 8144\n","[INFO]: Number of validation images: 8041\n","Computation device: cuda\n","Learning rate: 0.0001\n","Epochs to train for: 1\n","\n","[INFO]: Loading pre-trained weights\n","Loaded pretrained weights for efficientnet-b5\n","[INFO]: Fine-tuning all layers...\n","37,709,532 total parameters.\n","37,709,532 training parameters.\n","[INFO]: Epoch 1 of 1\n","0.0001\n","Training\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/255 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1f3348991d4ba58f34ad64506c9b33"}},"metadata":{}},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-14f7f8096069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     train_epoch_loss, train_epoch_acc = train(model, train_loader, \n\u001b[0m\u001b[1;32m     48\u001b[0m                                             optimizer, criterion)\n\u001b[1;32m     49\u001b[0m     valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n","\u001b[0;32m<ipython-input-9-d105f7db3728>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Calculate the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-e43caa1d5458>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# scale drop connect_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depthwise_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMemoryEfficientSwish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSwishImplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 14.76 GiB total capacity; 13.44 GiB already allocated; 11.75 MiB free; 13.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["Accuracy plots"],"metadata":{"id":"IIfbhXyQ3-2K"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 7))\n","plt.plot(train_acc, color='green', linestyle='-', label='train accuracy')\n","plt.plot(valid_acc, color='blue', linestyle='-', label='validataion accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"-h88Vnt44BGd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loss plots"],"metadata":{"id":"VpaagoDx4B_5"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 7))\n","plt.plot(train_loss, color='orange', linestyle='-', label='train loss')\n","plt.plot(valid_loss, color='red', linestyle='-', label='validataion loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"cq_m6jsP4DaH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Learning curves"],"metadata":{"id":"DtbHbBj34Mvz"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 7))\n","plt.plot(learning_rate, color='blue', linestyle='-', label='learning rate')\n","plt.xlabel('Epochs')\n","plt.ylabel('LR')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"CqG1neQD4Snz"},"execution_count":null,"outputs":[]}]}